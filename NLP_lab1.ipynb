{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyeIAd3gpUUWh7zu+0n/hS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alamodi123/NLP_LABS-/blob/main/NLP_lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "# Step 1: Load and Explore the Dataset\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "print(\"--- Available Texts in Gutenberg Corpus ---\")\n",
        "print(gutenberg.fileids())\n",
        "# Select a text\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "print(\"\\n--- First 500 Characters of the Text ---\")\n",
        "print(text[:500])\n",
        "# Step 2: Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "print(\"\\n--- Number of Sentences ---\")\n",
        "print(len(sentences))\n",
        "print(\"--- Number of Words ---\")\n",
        "print(len(words))\n",
        "# Step 3: Remove Stop Words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word.lower() for word in words if word.isalpha()\n",
        "and word.lower() not in stop_words]\n",
        "print(\"\\n--- First 20 Non-Stop Words ---\")\n",
        "print(filtered_words[:20])\n",
        "# Step 4: Word Frequency Analysis\n",
        "freq_dist = FreqDist(filtered_words)\n",
        "print(\"\\n--- 10 Most Common Words ---\")\n",
        "print(freq_dist.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YTpfA8WhYdO",
        "outputId": "a7f77421-ccd8-46cd-d755-5b6a24042335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Available Texts in Gutenberg Corpus ---\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "\n",
            "--- First 500 Characters of the Text ---\n",
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t\n",
            "\n",
            "--- Number of Sentences ---\n",
            "7493\n",
            "--- Number of Words ---\n",
            "191855\n",
            "\n",
            "--- First 20 Non-Stop Words ---\n",
            "['emma', 'jane', 'austen', 'volume', 'chapter', 'emma', 'woodhouse', 'handsome', 'clever', 'rich', 'comfortable', 'home', 'happy', 'disposition', 'seemed', 'unite', 'best', 'blessings', 'existence', 'lived']\n",
            "\n",
            "--- 10 Most Common Words ---\n",
            "[('emma', 860), ('could', 836), ('would', 818), ('miss', 599), ('must', 566), ('harriet', 500), ('much', 484), ('said', 483), ('one', 447), ('weston', 437)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jRhct2jTi-HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "092f8846",
        "outputId": "aeaca8a6-6ba1-4e4d-c7f4-dcd555e24da0"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Step 1: Load and Explore the Dataset (Same as original)\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "print(\"--- Available Texts in Gutenberg Corpus ---\")\n",
        "print(gutenberg.fileids())\n",
        "# Select a text\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "print(\"\\n--- First 500 Characters of the Text ---\")\n",
        "print(text[:500])\n",
        "\n",
        "# Step 2: Tokenization (Same as original)\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "print(\"\\n--- Number of Sentences ---\")\n",
        "print(len(sentences))\n",
        "print(\"--- Number of Words ---\")\n",
        "print(len(words))\n",
        "\n",
        "# Step 3: Word Frequency Analysis (Without Stop Word Removal)\n",
        "# We will use the 'words' list directly without filtering stop words\n",
        "# We will still filter for alphabetic words and convert to lowercase for consistency\n",
        "filtered_words_no_stopwords = [word.lower() for word in words if word.isalpha()]\n",
        "\n",
        "print(\"\\n--- First 20 Words (without Stop Word Removal) ---\")\n",
        "print(filtered_words_no_stopwords[:20])\n",
        "\n",
        "freq_dist_no_stopwords = FreqDist(filtered_words_no_stopwords)\n",
        "print(\"\\n--- 10 Most Common Words (without Stop Word Removal) ---\")\n",
        "print(freq_dist_no_stopwords.most_common(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Available Texts in Gutenberg Corpus ---\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "\n",
            "--- First 500 Characters of the Text ---\n",
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t\n",
            "\n",
            "--- Number of Sentences ---\n",
            "7493\n",
            "--- Number of Words ---\n",
            "191855\n",
            "\n",
            "--- First 20 Words (without Stop Word Removal) ---\n",
            "['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy']\n",
            "\n",
            "--- 10 Most Common Words (without Stop Word Removal) ---\n",
            "[('the', 5201), ('to', 5181), ('and', 4877), ('of', 4284), ('i', 3177), ('a', 3124), ('it', 2503), ('her', 2448), ('was', 2396), ('she', 2336)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "# Step 1: Load and Explore the Dataset\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "print(\"--- Available Texts in Gutenberg Corpus ---\")\n",
        "print(gutenberg.fileids())\n",
        "# Select a text\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "print(\"\\n--- First 500 Characters of the Text ---\")\n",
        "print(text[:500])\n",
        "# Step 2: Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "print(\"\\n--- Number of Sentences ---\")\n",
        "print(len(sentences))\n",
        "print(\"--- Number of Words ---\")\n",
        "# Step 4: Word Frequency Analysis\n",
        "freq_dist = FreqDist(filtered_words)\n",
        "print(\"\\n--- 10 Most Common Words ---\")\n",
        "print(freq_dist.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW2QlWTylLUV",
        "outputId": "b89e1291-b0d1-4d7b-9867-6ad46db2c49f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Available Texts in Gutenberg Corpus ---\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "\n",
            "--- First 500 Characters of the Text ---\n",
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t\n",
            "\n",
            "--- Number of Sentences ---\n",
            "7493\n",
            "--- Number of Words ---\n",
            "\n",
            "--- 10 Most Common Words ---\n",
            "[('emma', 860), ('could', 836), ('would', 818), ('miss', 599), ('must', 566), ('harriet', 500), ('much', 484), ('said', 483), ('one', 447), ('weston', 437)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FNd4jmgMle32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6b8ea17"
      },
      "source": [
        "### Task 1: Tokenization on a Bigger Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f651d31",
        "outputId": "ecbb2a94-b16f-4b2a-e7bc-deaedc85efa7"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load a larger text dataset (e.g., 'austen-emma.txt')\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "print(\"--- Tokenization on austen-emma.txt ---\")\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(f\"\\nNumber of Sentences: {len(sentences)}\")\n",
        "print(\"First 5 Sentences:\")\n",
        "for i, sent in enumerate(sentences[:5]):\n",
        "    print(f\"{i+1}: {sent[:100]}...\") # Print first 100 characters of each sentence\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(text)\n",
        "print(f\"\\nNumber of Words: {len(words)}\")\n",
        "print(\"First 20 Words:\")\n",
        "print(words[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tokenization on austen-emma.txt ---\n",
            "\n",
            "Number of Sentences: 7493\n",
            "First 5 Sentences:\n",
            "1: [Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a...\n",
            "2: She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in cons...\n",
            "3: Her mother\n",
            "had died too long ago for her to have more than an indistinct\n",
            "remembrance of her caresses...\n",
            "4: Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
            "less as a governess than a friend, ver...\n",
            "5: Between _them_ it was more the intimacy\n",
            "of sisters....\n",
            "\n",
            "Number of Words: 191855\n",
            "First 20 Words:\n",
            "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a0a8a93"
      },
      "source": [
        "### Task 2: Lemmatization on a Bigger Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4831c336",
        "outputId": "b3fb031b-32bb-46a1-b970-70037aef85e3"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt') # Needed for word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger') # Often helpful for better lemmatization\n",
        "\n",
        "# Load a larger text dataset\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"--- Lemmatization on austen-emma.txt ---\")\n",
        "\n",
        "# Perform lemmatization on the first 100 words for demonstration\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words[:100]]\n",
        "\n",
        "print(\"First 100 Lemmatized Words:\")\n",
        "print(lemmatized_words)\n",
        "\n",
        "# Note: Lemmatizing the entire text might take some time due to its size.\n",
        "# The above performs lemmatization on the first 100 words as an example."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Lemmatization on austen-emma.txt ---\n",
            "First 100 Lemmatized Words:\n",
            "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessing', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'year', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'She', 'wa', 'the', 'youngest', 'of', 'the', 'two', 'daughter', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'s\", 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', 'Her', 'mother', 'had', 'died']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fff29c73"
      },
      "source": [
        "### Task 3: Stop Word Removal on a Bigger Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f96b0355",
        "outputId": "00007147-cebd-429b-b93e-b000ba76c939"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt') # Needed for word_tokenize\n",
        "\n",
        "# Load a larger text dataset\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Get the English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "print(\"--- Stop Word Removal on austen-emma.txt ---\")\n",
        "\n",
        "# Remove stop words (converting to lowercase and keeping alphabetic words)\n",
        "filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "print(f\"\\nOriginal number of words: {len(words)}\")\n",
        "print(f\"Number of words after stop word removal (and filtering): {len(filtered_words)}\")\n",
        "print(\"\\nFirst 20 words after stop word removal:\")\n",
        "print(filtered_words[:20])\n",
        "\n",
        "# Note: Filtering for isalpha() and converting to lower() is done for better stop word matching and analysis."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Stop Word Removal on austen-emma.txt ---\n",
            "\n",
            "Original number of words: 191855\n",
            "Number of words after stop word removal (and filtering): 69693\n",
            "\n",
            "First 20 words after stop word removal:\n",
            "['emma', 'jane', 'austen', 'volume', 'chapter', 'emma', 'woodhouse', 'handsome', 'clever', 'rich', 'comfortable', 'home', 'happy', 'disposition', 'seemed', 'unite', 'best', 'blessings', 'existence', 'lived']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f81c795b"
      },
      "source": [
        "### Task 4: POS Tagging on a Bigger Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4081d69d",
        "outputId": "86581019-2eb3-4f7e-eed9-c43845b021ad"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt') # Needed for word_tokenize\n",
        "\n",
        "# Load a larger text dataset\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "print(\"--- POS Tagging on austen-emma.txt ---\")\n",
        "\n",
        "# Perform POS tagging on the first 100 words for demonstration\n",
        "pos_tags = nltk.pos_tag(words[:100])\n",
        "\n",
        "print(\"POS Tags for the first 100 words:\")\n",
        "print(pos_tags)\n",
        "\n",
        "# Note: POS tagging the entire text might take some time.\n",
        "# The above performs POS tagging on the first 100 words as an example."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- POS Tagging on austen-emma.txt ---\n",
            "POS Tags for the first 100 words:\n",
            "[('[', 'NNS'), ('Emma', 'NNP'), ('by', 'IN'), ('Jane', 'NNP'), ('Austen', 'NNP'), ('1816', 'CD'), (']', 'NNP'), ('VOLUME', 'NNP'), ('I', 'PRP'), ('CHAPTER', 'VBP'), ('I', 'PRP'), ('Emma', 'NNP'), ('Woodhouse', 'NNP'), (',', ','), ('handsome', 'NN'), (',', ','), ('clever', 'NN'), (',', ','), ('and', 'CC'), ('rich', 'JJ'), (',', ','), ('with', 'IN'), ('a', 'DT'), ('comfortable', 'JJ'), ('home', 'NN'), ('and', 'CC'), ('happy', 'JJ'), ('disposition', 'NN'), (',', ','), ('seemed', 'VBD'), ('to', 'TO'), ('unite', 'VB'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('blessings', 'NNS'), ('of', 'IN'), ('existence', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), ('lived', 'VBN'), ('nearly', 'RB'), ('twenty-one', 'CD'), ('years', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('with', 'IN'), ('very', 'RB'), ('little', 'JJ'), ('to', 'TO'), ('distress', 'VB'), ('or', 'CC'), ('vex', 'VB'), ('her', 'PRP'), ('.', '.'), ('She', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('youngest', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('two', 'CD'), ('daughters', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('most', 'RBS'), ('affectionate', 'JJ'), (',', ','), ('indulgent', 'JJ'), ('father', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), (',', ','), ('in', 'IN'), ('consequence', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sister', 'NN'), (\"'s\", 'POS'), ('marriage', 'NN'), (',', ','), ('been', 'VBN'), ('mistress', 'NN'), ('of', 'IN'), ('his', 'PRP$'), ('house', 'NN'), ('from', 'IN'), ('a', 'DT'), ('very', 'RB'), ('early', 'JJ'), ('period', 'NN'), ('.', '.'), ('Her', 'PRP$'), ('mother', 'NN'), ('had', 'VBD'), ('died', 'VBN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bee35ec2"
      },
      "source": [
        "### Task 5: Dependency Parsing on a Bigger Dataset (using spaCy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "24b840c0",
        "outputId": "63c21483-6c95-4af7-dddc-f564cbb48ca5"
      },
      "source": [
        "import spacy\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Load the spaCy English language model\n",
        "# If you encounter an error here, make sure you have installed spacy and downloaded the model:\n",
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except:\n",
        "    print(\"SpaCy model 'en_core_web_sm' not found. Please run the following commands in a new cell:\")\n",
        "    print(\"!pip install spacy\")\n",
        "    print(\"!python -m spacy download en_core_web_sm\")\n",
        "    nlp = None # Set nlp to None if the model is not loaded\n",
        "\n",
        "if nlp:\n",
        "    # Load a larger text dataset\n",
        "    text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "    print(\"--- Dependency Parsing on austen-emma.txt (First Sentence) ---\")\n",
        "\n",
        "    # Process the first sentence for dependency parsing as it's computationally intensive\n",
        "    # Using the first 500 characters to get a few sentences\n",
        "    first_sentences_text = text[:500]\n",
        "    doc = nlp(first_sentences_text)\n",
        "\n",
        "    # Perform dependency parsing and print relationships for the first few tokens\n",
        "    print(\"\\nDependency relationships for the first 20 tokens:\")\n",
        "    for token in doc[:20]:\n",
        "        print(f\"Word: {token.text}, Dependency: {token.dep_}, Head: {token.head.text}, POS: {token.pos_}\")\n",
        "\n",
        "    # Display the dependency tree for the first sentence (requires rendering in a Jupyter environment)\n",
        "    from spacy import displacy\n",
        "    print(\"\\nDependency Tree (for the first sentence):\")\n",
        "    # Find the first sentence to render its tree\n",
        "    first_sentence_doc = list(doc.sents)[0] if list(doc.sents) else doc\n",
        "    displacy.render(first_sentence_doc, style=\"dep\", jupyter=True)\n",
        "\n",
        "# Note: Dependency parsing and visualizing the entire text is not feasible in a notebook due to size and computational cost.\n",
        "# The example above processes and displays the first part of the text."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dependency Parsing on austen-emma.txt (First Sentence) ---\n",
            "\n",
            "Dependency relationships for the first 20 tokens:\n",
            "Word: [, Dependency: dep, Head: VOLUME, POS: X\n",
            "Word: Emma, Dependency: dep, Head: VOLUME, POS: PROPN\n",
            "Word: by, Dependency: prep, Head: Emma, POS: ADP\n",
            "Word: Jane, Dependency: compound, Head: Austen, POS: PROPN\n",
            "Word: Austen, Dependency: pobj, Head: by, POS: PROPN\n",
            "Word: 1816, Dependency: nummod, Head: Austen, POS: NUM\n",
            "Word: ], Dependency: punct, Head: VOLUME, POS: PUNCT\n",
            "Word: \n",
            "\n",
            ", Dependency: dep, Head: ], POS: SPACE\n",
            "Word: VOLUME, Dependency: ROOT, Head: VOLUME, POS: NOUN\n",
            "Word: I, Dependency: compound, Head: CHAPTER, POS: PRON\n",
            "Word: \n",
            "\n",
            ", Dependency: dep, Head: I, POS: SPACE\n",
            "Word: CHAPTER, Dependency: npadvmod, Head: VOLUME, POS: PROPN\n",
            "Word: I, Dependency: nsubj, Head: seemed, POS: PRON\n",
            "Word: \n",
            "\n",
            "\n",
            ", Dependency: dep, Head: I, POS: SPACE\n",
            "Word: Emma, Dependency: compound, Head: Woodhouse, POS: PROPN\n",
            "Word: Woodhouse, Dependency: appos, Head: I, POS: PROPN\n",
            "Word: ,, Dependency: punct, Head: Woodhouse, POS: PUNCT\n",
            "Word: handsome, Dependency: amod, Head: Woodhouse, POS: ADJ\n",
            "Word: ,, Dependency: punct, Head: handsome, POS: PUNCT\n",
            "Word: clever, Dependency: conj, Head: handsome, POS: ADJ\n",
            "\n",
            "Dependency Tree (for the first sentence):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"4fe4e8b5d1ae445580923ba6093ef82f-0\" class=\"displacy\" width=\"1975\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">[</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">X</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Emma</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">by</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Jane</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Austen</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">1816]</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PUNCT</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">\n",
              "\n",
              "</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">SPACE</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">VOLUME</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">I</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">\n",
              "\n",
              "</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">SPACE</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">CHAPTER</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 1275.0,2.0 1275.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,89.5 1270.0,89.5 1270.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-2\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M385.0,354.0 L393.0,342.0 377.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-4\" stroke-width=\"2px\" d=\"M420,352.0 C420,177.0 740.0,177.0 740.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M740.0,354.0 L748.0,342.0 732.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-5\" stroke-width=\"2px\" d=\"M945,352.0 C945,177.0 1265.0,177.0 1265.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,354.0 L937,342.0 953,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-6\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-7\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,177.0 1790.0,177.0 1790.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1470,354.0 L1462,342.0 1478,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-8\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,264.5 1610.0,264.5 1610.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1610.0,354.0 L1618.0,342.0 1602.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-9\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,89.5 1795.0,89.5 1795.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe4e8b5d1ae445580923ba6093ef82f-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1795.0,354.0 L1803.0,342.0 1787.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "163c7233"
      },
      "source": [
        "### Task 6: Named Entity Recognition on a Bigger Dataset (using spaCy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8e8f04b",
        "outputId": "9460954e-e086-4ab9-a2d2-06f28d515744"
      },
      "source": [
        "import spacy\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Load the spaCy English language model\n",
        "# Ensure spaCy and the model are installed as mentioned in Task 5\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except:\n",
        "    print(\"SpaCy model 'en_core_web_sm' not found. Please run the following commands in a new cell:\")\n",
        "    print(\"!pip install spacy\")\n",
        "    print(\"!python -m spacy download en_core_web_sm\")\n",
        "    nlp = None # Set nlp to None if the model is not loaded\n",
        "\n",
        "\n",
        "if nlp:\n",
        "    # Load a larger text dataset\n",
        "    text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "    print(\"--- Named Entity Recognition on austen-emma.txt (First 1000 characters) ---\")\n",
        "\n",
        "    # Process a portion of the text for NER as it's computationally intensive\n",
        "    doc = nlp(text[:1000]) # Process the first 1000 characters\n",
        "\n",
        "    print(\"\\nNamed Entities found in the first 1000 characters:\")\n",
        "    # Iterate over the named entities found in the document\n",
        "    if doc.ents:\n",
        "        for ent in doc.ents:\n",
        "            print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
        "    else:\n",
        "        print(\"No named entities found in the processed portion.\")\n",
        "\n",
        "# Note: Performing NER on the entire text might be computationally expensive.\n",
        "# The example above processes and displays entities from the first 1000 characters."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Named Entity Recognition on austen-emma.txt (First 1000 characters) ---\n",
            "\n",
            "Named Entities found in the first 1000 characters:\n",
            "Entity: Emma, Label: PERSON\n",
            "Entity: Jane Austen, Label: PERSON\n",
            "Entity: 1816, Label: DATE\n",
            "Entity: Emma Woodhouse, Label: PERSON\n",
            "Entity: two, Label: CARDINAL\n",
            "Entity: Sixteen years, Label: DATE\n",
            "Entity: Taylor, Label: PERSON\n",
            "Entity: Woodhouse, Label: PERSON\n",
            "Entity: Emma, Label: PERSON\n",
            "Entity: Taylor, Label: PERSON\n"
          ]
        }
      ]
    }
  ]
}